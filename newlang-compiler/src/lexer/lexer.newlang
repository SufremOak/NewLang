// This file contains the implementation of the lexer for NewLang.
// It defines the lexical analysis logic, including token definitions 
// and the function to tokenize input source code.

type TokenType = enum {
    IDENTIFIER,
    NUMBER,
    STRING,
    KEYWORD,
    SYMBOL,
    WHITESPACE,
    COMMENT,
    EOF
}

type Token = struct {
    type: TokenType,
    value: string,
    line: int,
    column: int
}

func isWhitespace(char: char) -> bool {
    return char == ' ' || char == '\t' || char == '\n' || char == '\r'
}

func isDigit(char: char) -> bool {
    return char >= '0' && char <= '9'
}

func isLetter(char: char) -> bool {
    return (char >= 'a' && char <= 'z') || (char >= 'A' && char <= 'Z') || char == '_'
}

func tokenize(input: string) -> list<Token> {
    tokens: list<Token> = []
    line: int = 1
    column: int = 1
    index: int = 0

    while index < input.length {
        char: char = input[index]

        if isWhitespace(char) {
            index += 1
            column += 1
            continue
        }

        if isDigit(char) {
            start: int = index
            while index < input.length && isDigit(input[index]) {
                index += 1
            }
            tokens.append(Token{NUMBER, input[start:index], line, column})
            column += index - start
            continue
        }

        if isLetter(char) {
            start: int = index
            while index < input.length && (isLetter(input[index]) || isDigit(input[index])) {
                index += 1
            }
            tokens.append(Token{IDENTIFIER, input[start:index], line, column})
            column += index - start
            continue
        }

        // Handle symbols and keywords
        // Add logic for handling symbols and keywords here

        index += 1
        column += 1
    }

    tokens.append(Token{EOF, "", line, column})
    return tokens
}